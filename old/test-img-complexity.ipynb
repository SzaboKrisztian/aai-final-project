{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "from utils import render_single, extract_best_entries, generate_pixel_columns, extract_random_entries\n",
    "from IPython.display import display, Image as IPImage\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from itertools import repeat\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 entries from ['./dataset/clock.ndjson', './dataset/bicycle.ndjson', './dataset/sailboat.ndjson', './dataset/house.ndjson', './dataset/car.ndjson']\n",
      "Done shuffling dataset\n"
     ]
    }
   ],
   "source": [
    "entries_per_cat = 1000\n",
    "\n",
    "data = []\n",
    "\n",
    "files = ['./dataset/clock.ndjson', './dataset/bicycle.ndjson', './dataset/sailboat.ndjson', './dataset/house.ndjson', './dataset/car.ndjson']\n",
    "test = [extract_random_entries(file, entries_per_cat, recognized=True) for file in files]\n",
    "flat_data = [item for sublist in test for item in sublist]\n",
    "df_loaded = pd.DataFrame.from_dict(flat_data, orient='columns')\n",
    "print(f'Loaded {len(df_loaded)} entries from {files}')\n",
    "df_test = df_loaded.sample(len(df_loaded))\n",
    "print('Done shuffling dataset')\n",
    "data.append(pd.concat([extract_best_entries(file, entries_per_cat, recognized=True) for file in files], ignore_index=True).sort_values(by='complexity'))\n",
    "data.append(pd.concat([extract_best_entries(file, entries_per_cat, recognized=True, descending=False) for file in files], ignore_index=True).sort_values(by='complexity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating pixel columns\n",
      "Done generating pixel columns\n"
     ]
    }
   ],
   "source": [
    "image_gen_params = {\n",
    "    'magnification': 4,\n",
    "    'resolution': 64,\n",
    "    'invert_color': True,\n",
    "    'stroke_width_scale': 2\n",
    "}\n",
    "\n",
    "result = []\n",
    "df_test = generate_pixel_columns(df_test, **image_gen_params).reset_index(drop=True)\n",
    "for d in data:\n",
    "    result.append({ 'data': generate_pixel_columns(d, **image_gen_params).reset_index(drop=True) })\n",
    "    print(f'Done generating pixel columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1/2...\n",
      "Done generating features and target (5000, 4096) Index(['word', 'countrycode', 'timestamp', 'recognized', 'key_id', 'drawing',\n",
      "       'complexity', 'pixel0', 'pixel1', 'pixel2',\n",
      "       ...\n",
      "       'pixel4086', 'pixel4087', 'pixel4088', 'pixel4089', 'pixel4090',\n",
      "       'pixel4091', 'pixel4092', 'pixel4093', 'pixel4094', 'pixel4095'],\n",
      "      dtype='object', length=4103)\n",
      "PCA & standardization done. Keeping 550 features\n",
      "Processing dataset 2/2...\n",
      "Done generating features and target (5000, 4096) Index(['word', 'countrycode', 'timestamp', 'recognized', 'key_id', 'drawing',\n",
      "       'complexity', 'pixel0', 'pixel1', 'pixel2',\n",
      "       ...\n",
      "       'pixel4086', 'pixel4087', 'pixel4088', 'pixel4089', 'pixel4090',\n",
      "       'pixel4091', 'pixel4092', 'pixel4093', 'pixel4094', 'pixel4095'],\n",
      "      dtype='object', length=4103)\n",
      "PCA & standardization done. Keeping 474 features\n"
     ]
    }
   ],
   "source": [
    "for i, entry in enumerate(result):\n",
    "    data = entry['data']\n",
    "    print(f'Processing dataset {i + 1}/{len(result)}...')\n",
    "    \n",
    "    y = data['word'].to_numpy()\n",
    "    X = data.drop(columns=['countrycode', 'timestamp', 'recognized', 'key_id', 'drawing', 'word', 'complexity']).to_numpy()\n",
    "    print(\"Done generating features and target\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    pca = PCA(.85)\n",
    "    X = pca.fit_transform(X)\n",
    "    print(f'PCA & standardization done. Keeping {pca.n_components_} features')\n",
    "    entry['scaler'] = scaler\n",
    "    entry['pca'] = pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training model in 38.63s\n",
      "Done training model in 60.86s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for entry in result:\n",
    "    train = entry['data']\n",
    "    pca = entry['pca']\n",
    "    scaler = entry['scaler']\n",
    "    classifier = MLPClassifier(hidden_layer_sizes=tuple(repeat(int(pca.n_components_ * 1.2), 3)), solver='lbfgs', alpha=1e-07)\n",
    "    y = train['word'].to_numpy()\n",
    "    X = train.drop(columns=['countrycode', 'timestamp', 'recognized', 'key_id', 'drawing', 'word', 'complexity']).to_numpy()\n",
    "    X = scaler.transform(X)\n",
    "    X = pca.transform(X)\n",
    "    entry['model'] = OneVsRestClassifier(classifier, n_jobs=-1).fit(X, y)\n",
    "    end = time.time()\n",
    "    print(f\"Done training model in {'{:.2f}'.format(end - start)}s\")\n",
    "    start = end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1 accuracy: 0.8458\n",
      "Dataset 2 accuracy: 0.7516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i, entry in enumerate(result):\n",
    "    scaler = entry['scaler']\n",
    "    pca = entry['pca']\n",
    "    model = entry['model']\n",
    "    test = df_test.drop(columns=['countrycode', 'timestamp', 'recognized', 'key_id', 'drawing', 'word']).to_numpy()\n",
    "    test = scaler.transform(test)\n",
    "    test = pca.transform(test)\n",
    "    prediction = model.predict(test)\n",
    "\n",
    "    acc_score = accuracy_score(df_test['word'].values.tolist(), prediction)\n",
    "    print(f\"Dataset {i+1} accuracy: {acc_score}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
