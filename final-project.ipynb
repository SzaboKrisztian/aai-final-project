{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "55bd4fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "from utils import render_single, render_multiple, get_dataset_files, extract_random_entries, extract_first_entries, generate_pixel_columns\n",
    "from IPython.display import display, Image as IPImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6c021e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12500 entries from ['./dataset/shorts.ndjson', './dataset/motorbike.ndjson', './dataset/envelope.ndjson', './dataset/suitcase.ndjson', './dataset/parrot.ndjson', './dataset/t-shirt.ndjson', './dataset/lighthouse.ndjson', './dataset/flip flops.ndjson', './dataset/speedboat.ndjson', './dataset/toothpaste.ndjson', './dataset/door.ndjson', './dataset/clarinet.ndjson', './dataset/lollipop.ndjson', './dataset/raccoon.ndjson', './dataset/microphone.ndjson', './dataset/mountain.ndjson', './dataset/traffic light.ndjson', './dataset/butterfly.ndjson', './dataset/spoon.ndjson', './dataset/spider.ndjson', './dataset/campfire.ndjson', './dataset/popsicle.ndjson', './dataset/frog.ndjson', './dataset/hot dog.ndjson', './dataset/elephant.ndjson']\n"
     ]
    }
   ],
   "source": [
    "num_cats = 25\n",
    "entries_per_cat = 500\n",
    "\n",
    "# files = get_dataset_files()\n",
    "# files = random.choices(files, k=num_cats)\n",
    "files = ['./dataset/shorts.ndjson', './dataset/motorbike.ndjson', './dataset/envelope.ndjson', './dataset/suitcase.ndjson', './dataset/parrot.ndjson', './dataset/t-shirt.ndjson', './dataset/lighthouse.ndjson', './dataset/flip flops.ndjson', './dataset/speedboat.ndjson', './dataset/toothpaste.ndjson', './dataset/door.ndjson', './dataset/clarinet.ndjson', './dataset/lollipop.ndjson', './dataset/raccoon.ndjson', './dataset/microphone.ndjson', './dataset/mountain.ndjson', './dataset/traffic light.ndjson', './dataset/butterfly.ndjson', './dataset/spoon.ndjson', './dataset/spider.ndjson', './dataset/campfire.ndjson', './dataset/popsicle.ndjson', './dataset/frog.ndjson', './dataset/hot dog.ndjson', './dataset/elephant.ndjson']\n",
    "data = [extract_first_entries(file, entries_per_cat, recognized=True) for file in files]\n",
    "flat_data = [item for sublist in data for item in sublist]\n",
    "df_loaded = pd.DataFrame.from_dict(flat_data, orient='columns')\n",
    "print(f'Loaded {len(df_loaded)} entries from {files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = random.choice(flat_data)\n",
    "display(IPImage(render_single(img['drawing'])))\n",
    "print(img['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58443c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1000 if entries_per_cat > 1000 else entries_per_cat\n",
    "word = random.choice(df_loaded['word'].values)\n",
    "imgs = df_loaded[df_loaded['word'] == word].sample(count)\n",
    "display(IPImage(render_multiple(imgs['drawing'])))\n",
    "print(f'{count} superimposed {word}s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4dd1514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done shuffling dataset\n",
      "Done generating pixel columns\n",
      "Train: 11250 entries, test: 1250 entries.\n"
     ]
    }
   ],
   "source": [
    "df = df_loaded.sample(len(df_loaded))\n",
    "print('Done shuffling dataset')\n",
    "df = generate_pixel_columns(df, resolution=64, invert_color=True, stroke_width_scale=2)\n",
    "print('Done generating pixel columns')\n",
    "df = df.reset_index()\n",
    "\n",
    "train_amt = int(len(df) * .9)\n",
    "\n",
    "train = df[:train_amt]\n",
    "test = df[train_amt:]\n",
    "# del df\n",
    "\n",
    "train = train.reset_index()\n",
    "test = test.reset_index()\n",
    "\n",
    "print(f'Train: {len(train)} entries, test: {len(test)} entries.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e4275f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving dataset to disk\n",
      "Keeping 640 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca_on = True\n",
    "save_to_disk = True\n",
    "\n",
    "if save_to_disk:\n",
    "    stamp = str(int(time.time()))\n",
    "    folder = f'./runs/{stamp}/'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    pd.DataFrame.to_feather(df, folder + 'data')\n",
    "    print('Done saving dataset to disk')\n",
    "\n",
    "y = train['word']\n",
    "X = train.drop(columns=['countrycode', 'timestamp', 'recognized', 'key_id', 'drawing', 'word'])\n",
    "\n",
    "if pca_on:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    pca = PCA(.85)\n",
    "    X = pca.fit_transform(X)\n",
    "    print(f'Keeping {pca.n_components_} features')\n",
    "    if save_to_disk:\n",
    "        joblib.dump(pca, folder + 'pca')\n",
    "        joblib.dump(pca, folder + 'scaler')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050407b2",
   "metadata": {},
   "source": [
    "```\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=SVC(),\n",
    "    param_grid={\n",
    "        'C': [1, 3, 15, 70, 200, 1000],\n",
    "        'gamma': [.009, .01, .5, 1, 1.5, 3, 6]\n",
    "    },\n",
    "    refit=True,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X,y)\n",
    "\n",
    "print('Best hyperparameters:', grid.best_params_)\n",
    "```\n",
    "\n",
    "Best hyperparameters: {'C': 3, 'gamma': 0.009}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "585bd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# classifier = LinearSVC(random_state=0, max_iter=100000, dual=False)\n",
    "# classifier = NuSVC(nu=.1, max_iter=10000)\n",
    "# classifier = SGDClassifier(loss='epsilon_insensitive', penalty='elasticnet', n_jobs=-1)\n",
    "classifier = SVC(kernel='rbf', C=3, gamma=.0)\n",
    "model = OneVsRestClassifier(classifier, n_jobs=-1).fit(X, y)\n",
    "if save_to_disk: joblib.dump(model, folder + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test.sample(1)\n",
    "sample_predict = sample.drop(columns=['countrycode', 'timestamp', 'recognized', 'key_id', 'drawing', 'word'])\n",
    "\n",
    "if pca_on:\n",
    "    sample_predict = scaler.transform(sample_predict)\n",
    "    sample_predict = pca.transform(sample_predict)\n",
    "\n",
    "prediction = model.predict(sample_predict)\n",
    "display(IPImage(render_single(sample['drawing'].iloc[0])))\n",
    "print(prediction[0])\n",
    "print(f\"{sample['word'].iloc[0]} == {prediction[0]} ? {sample['word'].iloc[0] == prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bc0d05d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.0416\n"
     ]
    }
   ],
   "source": [
    "test2 = test.drop(columns=['countrycode', 'timestamp', 'recognized', 'key_id', 'drawing', 'word'])\n",
    "if pca_on:\n",
    "    test2 = scaler.transform(test2)\n",
    "    test2 = pca.transform(test2)\n",
    "prediction = model.predict(test2)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_score = accuracy_score(test['word'].values.tolist(), prediction)\n",
    "print(f\"Accuracy score: {acc_score}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
